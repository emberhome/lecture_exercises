{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5950dbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 16:14:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/17 16:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:14:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:15:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:15:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:15:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/06/17 16:15:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------------+-------------------+\n",
      "|       customer_city|num_customers|            avg_lat|            avg_lng|\n",
      "+--------------------+-------------+-------------------+-------------------+\n",
      "|      rio de janeiro|       940474|-22.924669582662343|-43.311143011948246|\n",
      "|      belo horizonte|       462213| -19.91665353565284|-43.953073195699275|\n",
      "|             niteroi|       201442|-22.892451533078965| -43.09423974641364|\n",
      "|            curitiba|       161210| -25.45070305670467|-49.273743126594546|\n",
      "|          uberlandia|       154390|-18.917304147832297| -48.27264315216382|\n",
      "|        porto alegre|       141845|-30.048784899561856|  -51.1961082468199|\n",
      "|              santos|       134971|  -23.9649817735338| -46.32551909770966|\n",
      "|            campinas|       125935| -22.90420474635976| -47.07866709061551|\n",
      "|         santo andre|       115601|-23.655537698697035| -46.52248010874211|\n",
      "|            salvador|        94805|-12.977160380940527|-38.460716254893526|\n",
      "|sao bernardo do c...|        94044| -23.71406396712161| -46.56101006982038|\n",
      "|       montes claros|        89193|-16.727035487774288| -43.86255020082331|\n",
      "|            ipatinga|        88437|-19.470197477841694|-42.557192202496125|\n",
      "|           guarulhos|        79698| -23.43829366702017|-46.486998239168614|\n",
      "|       florianopolis|        78918|-27.562918551296924|-48.507445351847906|\n",
      "| sao jose dos campos|        78494|-23.216416019701906|-45.887746862674724|\n",
      "|         divinopolis|        76906| -20.14219282913941| -44.88597753319159|\n",
      "|             jundiai|        74314|-23.177977140987295| -46.91622764370128|\n",
      "|            brasilia|        65899| -15.81403783216322|  -47.9964023686437|\n",
      "|             vitoria|        65795|-20.283920397283655| -40.29063450820097|\n",
      "+--------------------+-------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, broadcast, sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OptimizationExample\").master(\"local[4]\").config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "\n",
    "geo_df = spark.read.csv(\"olist_geolocation_dataset.csv\", header=True)\n",
    "customer_df = spark.read.csv(\"olist_customers_dataset.csv\", header=True)\n",
    "\n",
    "joined_df = geo_df.join(customer_df, geo_df.geolocation_zip_code_prefix == customer_df.customer_zip_code_prefix)\n",
    "\n",
    "distinct_df = joined_df.distinct()\n",
    "\n",
    "aggregated = distinct_df.groupBy(\"customer_city\").agg(\n",
    "    count(\"customer_id\").alias(\"num_customers\"),\n",
    "    avg(\"geolocation_lat\").alias(\"avg_lat\"),\n",
    "    avg(\"geolocation_lng\").alias(\"avg_lng\")\n",
    ")\n",
    "final_df = aggregated.filter(aggregated.customer_city != \"sao paulo\")\n",
    "sorted_df = final_df.sort(col(\"num_customers\").desc())\n",
    "sorted_df.show()\n",
    "input(\"press enter to end spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74356e00",
   "metadata": {},
   "source": [
    "# Optimizing the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a176ff6",
   "metadata": {},
   "source": [
    "## a. use parquet instead of csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea00680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 16:22:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CSVtoParquet\").master(\"local[4]\").getOrCreate()\n",
    "\n",
    "geo_df = spark.read.csv(\"olist_geolocation_dataset.csv\", header=True)\n",
    "customer_df = spark.read.csv(\"olist_customers_dataset.csv\", header=True)\n",
    "\n",
    "geo_df.write.parquet(\"olist_geolocation_dataset.parquet\")\n",
    "#geo_df.write.mode('overwrite').parquet(\"olist_geolocation_dataset.parquet\") is another option\n",
    "customer_df.write.parquet(\"olist_customers_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f793f3",
   "metadata": {},
   "source": [
    "## b. filter earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ad8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = customer_df.filter(customer_df.customer_city != \"sao paulo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2cf9e",
   "metadata": {},
   "source": [
    "## c. aggregate earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a31c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_agg = geo_df.groupBy(\"geolocation_city\").agg(\n",
    "    avg(\"geolocation_lat\").alias(\"city_avg_lat\"),\n",
    "    avg(\"geolocation_lng\").alias(\"city_avg_lng\")\n",
    ")\n",
    "\n",
    "customer_agg = customer_df.groupBy(\"customer_city\").agg(\n",
    "    count(\"customer_id\").alias(\"num_customers\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ceac64",
   "metadata": {},
   "source": [
    "## d. broadcast join instead of regular join (when 1 side of join is much smaller than the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5772f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = geo_agg.join(broadcast(customer_agg), geo_agg.geolocation_city == customer_agg.customer_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f3be2",
   "metadata": {},
   "source": [
    "## e. cache to prevent re-calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1689c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[geolocation_city: string, city_avg_lat: double, city_avg_lng: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_agg.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06061e",
   "metadata": {},
   "source": [
    "## New & Optimized Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e1e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 16:49:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+-------------+\n",
      "|       city_avg_lat|       city_avg_lng|       customer_city|num_customers|\n",
      "+-------------------+-------------------+--------------------+-------------+\n",
      "|-22.914910090595374| -43.31288271552523|      rio de janeiro|         6882|\n",
      "|  -19.9087721380183|-43.957549373181315|      belo horizonte|         2773|\n",
      "|-15.811072659488262|-47.971704877885685|            brasilia|         2131|\n",
      "|-25.453054020386823| -49.27499629427101|            curitiba|         1521|\n",
      "|-22.900861066020287|-47.068536074738404|            campinas|         1444|\n",
      "|-30.049252267575962| -51.18858153639673|        porto alegre|         1379|\n",
      "|-12.962654589682149|  -38.4616830422837|            salvador|         1245|\n",
      "|-23.446403450611076| -46.49876846180308|           guarulhos|         1189|\n",
      "| -23.70762633840408| -46.56386871942135|sao bernardo do c...|          938|\n",
      "|-22.893927051796183|-43.086335277840156|             niteroi|          849|\n",
      "|-23.655240519546762|-46.519084194684474|         santo andre|          797|\n",
      "|-23.536560947809892| -46.78828362886693|              osasco|          746|\n",
      "| -23.96252319074526| -46.32780350647331|              santos|          713|\n",
      "| -16.68458663973802| -49.27448045931699|             goiania|          692|\n",
      "| -23.21087970704398|-45.885186496536896| sao jose dos campos|          691|\n",
      "| -3.763107946625512| -38.53242972497402|           fortaleza|          654|\n",
      "|-23.485346151382082| -47.46445000569193|            sorocaba|          633|\n",
      "| -8.068138874398839| -34.91363210482967|              recife|          613|\n",
      "| -27.57683563925412|-48.517951844329865|       florianopolis|          570|\n",
      "|-23.185675744261623| -46.90259409644331|             jundiai|          565|\n",
      "+-------------------+-------------------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"OptimizationExample\").master(\"local[4]\").config(\"spark.driver.memory\", \"2g\").getOrCreate()\n",
    "geo_df = spark.read.parquet(\"olist_geolocation_dataset.parquet\", header=True)\n",
    "customer_df = spark.read.parquet(\"olist_customers_dataset.parquet\", header=True)\n",
    "customer_df = customer_df.filter(customer_df.customer_city != \"sao paulo\")\n",
    "geo_agg = geo_df.groupBy(\"geolocation_city\").agg(\n",
    "    avg(\"geolocation_lat\").alias(\"city_avg_lat\"),\n",
    "    avg(\"geolocation_lng\").alias(\"city_avg_lng\")\n",
    ")\n",
    "customer_agg = customer_df.groupBy(\"customer_city\").agg(\n",
    "    count(\"customer_id\").alias(\"num_customers\")\n",
    ")\n",
    "joined_df = geo_agg.join(broadcast(customer_agg), geo_agg.geolocation_city == customer_agg.customer_city)\n",
    "joined_df = joined_df.drop(\"geolocation_city\")\n",
    "sorted_df = joined_df.sort(col(\"num_customers\").desc())\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0714f",
   "metadata": {},
   "source": [
    "### went from 5 minutes to a few seconds!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9aa619",
   "metadata": {},
   "source": [
    "## Advanced Optimization - Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb3dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing data partitioned by 'customer_state'\n",
    "customer_df.write.partitionBy(\"customer_state\").parquet(\"partitioned_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e78883",
   "metadata": {},
   "source": [
    "## Advanced Optimization - Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254bad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Writing data bucketed by 'customer_zip_code_prefix' into 50 buckets\n",
    "customer_df.write.bucketBy(50, \"customer_zip_code_prefix\").saveAsTable(\"bucketed_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64513e2",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363d8f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 17:14:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MLExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3a9918e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+------+\n",
      "| price|bedrooms|surface|floors|\n",
      "+------+--------+-------+------+\n",
      "| 274.0|       3|   1830|   2.0|\n",
      "| 500.0|       4|   2120|   1.0|\n",
      "| 320.0|       3|   1260|   1.0|\n",
      "| 445.5|       3|   1880|   1.0|\n",
      "| 637.5|       3|   1680|   1.0|\n",
      "| 460.0|       2|   2730|   1.0|\n",
      "| 259.0|       3|   1270|   1.5|\n",
      "| 950.0|       3|   2780|   1.0|\n",
      "| 550.0|       3|   1930|   2.0|\n",
      "| 265.5|       3|   1860|   1.0|\n",
      "| 162.0|       4|   1460|   1.0|\n",
      "|2395.0|       4|   3800|   2.0|\n",
      "| 385.0|       3|   1070|   1.0|\n",
      "| 230.0|       3|   1010|   1.0|\n",
      "| 665.0|       3|   1940|   1.5|\n",
      "| 412.0|       4|   3360|   2.0|\n",
      "| 177.5|       3|   1220|   1.0|\n",
      "| 330.0|       4|   2000|   1.0|\n",
      "| 445.0|       4|   2430|   1.5|\n",
      "| 139.5|       2|   1230|   2.0|\n",
      "+------+--------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"flats.csv\", header=True, inferSchema=True)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f8af004",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=[\"bedrooms\", \"surface\", \"floors\"], outputCol=\"features\")\n",
    "data_assembled = vector_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78695901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b7f3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 17:18:37 WARN Instrumentation: [186aebd4] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/06/17 17:18:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/06/17 17:18:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(data_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 240.50644355361382\n",
      "Mean Absolute Error (MAE): 164.4301643578809\n",
      "R^2: 0.5422973289292989\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = lr_model.transform(data_assembled)\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\") #prediction is about $240 off from the real price on average\n",
    "\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")  #similar to above\n",
    "\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(f\"R^2: {r2}\") # 54% of variability in Y can be explained by model, ideally it would be 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6068cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
